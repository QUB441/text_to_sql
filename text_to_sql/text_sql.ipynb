{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.9)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language to SQL transformers \n",
    "#### with graph relationships Graphix T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's exlore the data\n",
    "import datasets\n",
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"xlangai/spider\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'db_id': 'department_management', 'query': 'SELECT count(*) FROM head WHERE age  >  56', 'question': 'How many heads of the departments are older than 56 ?', 'query_toks': ['SELECT', 'count', '(', '*', ')', 'FROM', 'head', 'WHERE', 'age', '>', '56'], 'query_toks_no_value': ['select', 'count', '(', '*', ')', 'from', 'head', 'where', 'age', '>', 'value'], 'question_toks': ['How', 'many', 'heads', 'of', 'the', 'departments', 'are', 'older', 'than', '56', '?']}\n"
     ]
    }
   ],
   "source": [
    "print(ds['train'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(ds['train'])\n",
    "\n",
    "df_test = pd.DataFrame(ds['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>db_id</th>\n",
       "      <th>query</th>\n",
       "      <th>question</th>\n",
       "      <th>query_toks</th>\n",
       "      <th>query_toks_no_value</th>\n",
       "      <th>question_toks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>department_management</td>\n",
       "      <td>SELECT count(*) FROM head WHERE age  &gt;  56</td>\n",
       "      <td>How many heads of the departments are older th...</td>\n",
       "      <td>[SELECT, count, (, *, ), FROM, head, WHERE, ag...</td>\n",
       "      <td>[select, count, (, *, ), from, head, where, ag...</td>\n",
       "      <td>[How, many, heads, of, the, departments, are, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>department_management</td>\n",
       "      <td>SELECT name ,  born_state ,  age FROM head ORD...</td>\n",
       "      <td>List the name, born state and age of the heads...</td>\n",
       "      <td>[SELECT, name, ,, born_state, ,, age, FROM, he...</td>\n",
       "      <td>[select, name, ,, born_state, ,, age, from, he...</td>\n",
       "      <td>[List, the, name, ,, born, state, and, age, of...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   db_id                                              query  \\\n",
       "0  department_management         SELECT count(*) FROM head WHERE age  >  56   \n",
       "1  department_management  SELECT name ,  born_state ,  age FROM head ORD...   \n",
       "\n",
       "                                            question  \\\n",
       "0  How many heads of the departments are older th...   \n",
       "1  List the name, born state and age of the heads...   \n",
       "\n",
       "                                          query_toks  \\\n",
       "0  [SELECT, count, (, *, ), FROM, head, WHERE, ag...   \n",
       "1  [SELECT, name, ,, born_state, ,, age, FROM, he...   \n",
       "\n",
       "                                 query_toks_no_value  \\\n",
       "0  [select, count, (, *, ), from, head, where, ag...   \n",
       "1  [select, name, ,, born_state, ,, age, from, he...   \n",
       "\n",
       "                                       question_toks  \n",
       "0  [How, many, heads, of, the, departments, are, ...  \n",
       "1  [List, the, name, ,, born, state, and, age, of...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159    [[-1, *], [0, department id], [0, name], [0, creation], [0, ranking], [0, budget in billions], [0, num employees], [1, head id], [1, name], [1, born state], [1, age], [2, department id], [2, head id], [2, temporary acting]]\n",
      "Name: column_names, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Set display options to see the full output\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "tables = pd.read_json('tables.json')\n",
    "\n",
    "# Now print the DataFrame\n",
    "print(tables[tables['db_id'] == 'department_management']['column_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_schema_for_db(db_id, tables_df):\n",
    "    # Find the schema for the specific database ID\n",
    "    \n",
    "    tables_data = tables_df.to_dict(orient='records')\n",
    "    schema = next((db for db in tables_data if db[\"db_id\"] == db_id), None)\n",
    "    if not schema:\n",
    "        raise ValueError(f\"Schema for db_id {db_id} not found.\")\n",
    "\n",
    "    # Extract tables and columns\n",
    "    tables = schema[\"table_names\"]\n",
    "    columns = schema[\"column_names\"]\n",
    "    primary_keys = schema[\"primary_keys\"]\n",
    "    foreign_keys = schema[\"foreign_keys\"]\n",
    "\n",
    "    # Prepare nodes and edges for the graph\n",
    "    nodes = tables + [f\"{col[1]}\" for col in columns if col[0] != -1]  # Exclude global columns\n",
    "    edges = []\n",
    "\n",
    "    # Table-Column edges\n",
    "    for col in columns:\n",
    "        if col[0] != -1:  # Skip global columns\n",
    "            edges.append((tables[col[0]], col[1]))\n",
    "\n",
    "    # Primary Key edges\n",
    "    for pk in primary_keys:\n",
    "        edges.append((f\"{columns[pk][1]}\", f\"{columns[pk][1]} (PK)\"))\n",
    "\n",
    "    # Foreign Key edges\n",
    "    for fk in foreign_keys:\n",
    "        from_col = f\"{columns[fk[0]][1]}\"\n",
    "        to_col = f\"{columns[fk[1]][1]}\"\n",
    "        edges.append((from_col, to_col))\n",
    "\n",
    "    return {\"nodes\": nodes, \"edges\": edges}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nodes': ['department',\n",
       "  'head',\n",
       "  'management',\n",
       "  'department id',\n",
       "  'name',\n",
       "  'creation',\n",
       "  'ranking',\n",
       "  'budget in billions',\n",
       "  'num employees',\n",
       "  'head id',\n",
       "  'name',\n",
       "  'born state',\n",
       "  'age',\n",
       "  'department id',\n",
       "  'head id',\n",
       "  'temporary acting'],\n",
       " 'edges': [('department', 'department id'),\n",
       "  ('department', 'name'),\n",
       "  ('department', 'creation'),\n",
       "  ('department', 'ranking'),\n",
       "  ('department', 'budget in billions'),\n",
       "  ('department', 'num employees'),\n",
       "  ('head', 'head id'),\n",
       "  ('head', 'name'),\n",
       "  ('head', 'born state'),\n",
       "  ('head', 'age'),\n",
       "  ('management', 'department id'),\n",
       "  ('management', 'head id'),\n",
       "  ('management', 'temporary acting'),\n",
       "  ('department id', 'department id (PK)'),\n",
       "  ('head id', 'head id (PK)'),\n",
       "  ('department id', 'department id (PK)'),\n",
       "  ('head id', 'head id'),\n",
       "  ('department id', 'department id')]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage\n",
    "db_id = \"department_management\"\n",
    "\n",
    "schema_graph = extract_schema_for_db(db_id, tables)\n",
    "schema_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_t5_inputs_targets(data, tables_data):\n",
    "    # Convert data to list of dictionaries if it's a DataFrame\n",
    "    if hasattr(data, \"to_dict\"):\n",
    "        data = data.to_dict(orient=\"records\")\n",
    "\n",
    "    inputs = []\n",
    "    targets = []\n",
    "\n",
    "    for record in data:\n",
    "        db_id = record[\"db_id\"]\n",
    "        question = record[\"question\"]\n",
    "        query = record[\"query\"]\n",
    "\n",
    "        # Extract schema for the db_id\n",
    "        schema = extract_schema_for_db(db_id, tables_data)\n",
    "\n",
    "        # Serialize the schema\n",
    "        tables = \", \".join(schema[\"nodes\"][: len(schema[\"nodes\"]) // 2])\n",
    "        columns = \", \".join([node for node in schema[\"nodes\"] if '.' in node])\n",
    "        primary_keys = \", \".join([f\"{edge[0]} (PK)\" for edge in schema[\"edges\"] if \"(PK)\" in edge[0]])\n",
    "        foreign_keys = \", \".join([f\"{edge[0]} -> {edge[1]}\" for edge in schema[\"edges\"] if \"->\" in edge[1]])\n",
    "\n",
    "        serialized_schema = (\n",
    "            f\"Tables: {tables} | Columns: {columns} | \"\n",
    "            f\"Primary Keys: {primary_keys} | Foreign Keys: {foreign_keys}\"\n",
    "        )\n",
    "\n",
    "        # Prepare input and target\n",
    "        input_text = f\"translate natural language to SQL: {question}? <schema> {serialized_schema}\"\n",
    "        target_text = query\n",
    "\n",
    "        inputs.append(input_text)\n",
    "        targets.append(target_text)\n",
    "\n",
    "    return inputs, targets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's test on a subset\n",
    "data = df\n",
    "inputs, targets = prepare_t5_inputs_targets(data, tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create text class\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TextToSQLDataset(Dataset):\n",
    "    def __init__(self, inputs, targets, tokenizer, max_length=512):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_text = self.inputs[idx]\n",
    "        target_text = self.targets[idx]\n",
    "\n",
    "        # Tokenize inputs and targets\n",
    "        input_encodings = self.tokenizer(\n",
    "            input_text,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        target_encodings = self.tokenizer(\n",
    "            target_text,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        # Replace padding token id with -100 in labels\n",
    "        labels = target_encodings[\"input_ids\"]\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_encodings[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": input_encodings[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": labels.squeeze(0),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AdamW\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "# Initialize Tokenizer and Model\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split data into train and validation\n",
    "train_inputs, val_inputs, train_targets, val_targets = train_test_split(\n",
    "    inputs, targets, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "#Prepare Datasets\n",
    "train_dataset = TextToSQLDataset(train_inputs, train_targets, tokenizer)\n",
    "val_dataset = TextToSQLDataset(val_inputs, val_targets, tokenizer)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Move Model to GPU if Available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mliquid-candidate\u001b[0m (\u001b[33mliquid-candidate-personal\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/project/text_to_sql/wandb/run-20241204_150038-7rfpnpfv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/liquid-candidate-personal/text-to-sql/runs/7rfpnpfv' target=\"_blank\">t5-finetuning</a></strong> to <a href='https://wandb.ai/liquid-candidate-personal/text-to-sql' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/liquid-candidate-personal/text-to-sql' target=\"_blank\">https://wandb.ai/liquid-candidate-personal/text-to-sql</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/liquid-candidate-personal/text-to-sql/runs/7rfpnpfv' target=\"_blank\">https://wandb.ai/liquid-candidate-personal/text-to-sql/runs/7rfpnpfv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/liquid-candidate-personal/text-to-sql/runs/7rfpnpfv?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x779bad1e3250>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize W&B\n",
    "wandb.init(\n",
    "    project=\"text-to-sql\", \n",
    "    name=\"t5-finetuning\", \n",
    "    config={\n",
    "        \"epochs\": 5,\n",
    "        \"batch_size\": 16,\n",
    "        \"learning_rate\": 5e-5,\n",
    "        \"max_length\": 512,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 1.4201\n",
      "Validation Loss: 0.6996\n",
      "Epoch 2/5, Loss: 0.7121\n",
      "Epoch 2: SELECT T1.name FROM ENG\n",
      "Validation Loss: 0.4896\n",
      "Epoch 3/5, Loss: 0.5180\n",
      "Validation Loss: 0.3943\n",
      "Epoch 4/5, Loss: 0.4150\n",
      "Epoch 4: SELECT Name FROM Employees\n",
      "Validation Loss: 0.3443\n",
      "Epoch 5/5, Loss: 0.3459\n",
      "Validation Loss: 0.2924\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch_loss</td><td>█▇▅▅▁</td></tr><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>epoch_loss</td><td>█▃▂▁▁</td></tr><tr><td>val_loss</td><td>█▄▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch_loss</td><td>0.06244</td></tr><tr><td>epoch</td><td>5</td></tr><tr><td>epoch_loss</td><td>0.34594</td></tr><tr><td>val_loss</td><td>0.29238</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">t5-finetuning</strong> at: <a href='https://wandb.ai/liquid-candidate-personal/text-to-sql/runs/7rfpnpfv' target=\"_blank\">https://wandb.ai/liquid-candidate-personal/text-to-sql/runs/7rfpnpfv</a><br/> View project at: <a href='https://wandb.ai/liquid-candidate-personal/text-to-sql' target=\"_blank\">https://wandb.ai/liquid-candidate-personal/text-to-sql</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241204_150038-7rfpnpfv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training Loop\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    # Training Phase\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Move batch to device\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Verify labels\n",
    "       # print(\"Sample labels:\", labels[0])  # Check if padding is replaced with -100\n",
    "\n",
    "        # Forward Pass\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backward Pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    wandb.log({\"batch_loss\": loss.item()})\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Debug the model's output every 10 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            inputs = tokenizer(\"Translate this English question into SQL: What are the names of all employees?\", return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "            outputs = model.generate(inputs[\"input_ids\"], max_length=20)\n",
    "            print(f\"Epoch {epoch + 1}: {tokenizer.decode(outputs[0], skip_special_tokens=True)}\")\n",
    "\n",
    "    # Log epoch loss to W&B\n",
    "    wandb.log({\"epoch\": epoch + 1, \"epoch_loss\": avg_loss})\n",
    "\n",
    "    # Validation Phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            # Move batch to device\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            # Forward Pass\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            val_loss += outputs.loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Log validation loss to W&B\n",
    "    wandb.log({\"val_loss\": avg_val_loss})\n",
    "\n",
    "#inish W&B Logging\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_inference(question, model, tokenizer, device):\n",
    "    \"\"\"\n",
    "    Run inference on a single question to generate an SQL query.\n",
    "\n",
    "    Args:\n",
    "    - question (str): The natural language question.\n",
    "    - model (T5ForConditionalGeneration): The fine-tuned T5 model.\n",
    "    - tokenizer (T5Tokenizer): The T5 tokenizer.\n",
    "    - device (torch.device): The device to run the model on.\n",
    "\n",
    "    Returns:\n",
    "    - str: The generated SQL query.\n",
    "    \"\"\"\n",
    "    # Add the task prefix\n",
    "    #input_text = f\"translate natural language to SQL: {question}\"\n",
    "    \n",
    "    prefix = \"translate natural language to SQL:\"\n",
    "    input_text = prefix + question\n",
    "\n",
    "    # Tokenize the input\n",
    "    input_ids = tokenizer(\n",
    "        input_text,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "    ).input_ids.to(device)\n",
    "    \n",
    "    print(\"Tokenized input text:\", tokenizer.convert_ids_to_tokens(input_ids[0]))\n",
    "    print(\"Input ids:\", input_ids)\n",
    "    print(type(input_ids))\n",
    "    # Ensure model is in eval mode\n",
    "    model.eval()\n",
    "\n",
    "    # Generate SQL query\n",
    "    outputs = model.generate(input_ids, max_length=128, num_beams=5, early_stopping=True)\n",
    "    \n",
    "    print(\"Generated output tensor:\", outputs)\n",
    "\n",
    "    # Decode the output tokens\n",
    "    sql_query = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(\"Decoded SQL query:\", sql_query)\n",
    "\n",
    "    return sql_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized input text: ['▁translate', '▁natural', '▁language', '▁to', '▁SQL', ':', 'get', '▁all', '▁student', '▁names', '</s>']\n",
      "Input ids: tensor([[13959,   793,  1612,    12, 12558,    10,  2782,    66,  1236,  3056,\n",
      "             1]], device='cuda:0')\n",
      "<class 'torch.Tensor'>\n",
      "Generated output tensor: tensor([[    0,     3, 23143, 14196,   564, 21680,  5097, 10161,  6431,     1]],\n",
      "       device='cuda:0')\n",
      "Decoded SQL query: SELECT name FROM STUDENT\n",
      "Input Question: get all student names\n",
      "Generated SQL Query: SELECT name FROM STUDENT\n"
     ]
    }
   ],
   "source": [
    "#example_question = df.iloc[0][\"question\"]  # First question in the dataset\n",
    "#print(example_question)\n",
    "example_question = \"get all student names\"\n",
    "#expected_query = df.iloc[0][\"query\"]      # First query in the dataset\n",
    "# Run Inference\n",
    "model.eval()\n",
    "generated_query = test_inference(example_question, model, tokenizer, device)\n",
    "print(f\"Input Question: {example_question}\")\n",
    "#print(f\"Expected Query: {expected_query}\")\n",
    "print(f\"Generated SQL Query: {generated_query}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model weights \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
